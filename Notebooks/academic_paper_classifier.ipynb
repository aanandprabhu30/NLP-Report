{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üéì Academic Paper Classifier v7.0\n",
        "\n",
        "**Automatically classify academic papers by discipline, subfield, and research methodology**\n",
        "\n",
        "This notebook provides a complete, production-ready classification system for academic papers. Simply input a title and abstract, and get back the paper's:\n",
        "\n",
        "- **üéØ Discipline**: Computer Science (CS), Information Systems (IS), or Information Technology (IT)\n",
        "- **üî¨ Subfield**: Specialized area within the discipline (e.g., AI/ML, Security, DevOps, etc.)\n",
        "- **üìä Methodology**: Research approach (Qualitative, Quantitative, or Mixed)\n",
        "\n",
        "## üöÄ Quick Start\n",
        "1. **Run cells 1-5** (Setup & Test)\n",
        "2. **Jump to Section 3** for examples  \n",
        "3. **Use Section 4** for your own papers\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üìö Classification Categories Explained\n",
        "\n",
        "### üéØ Disciplines\n",
        "| **Code** | **Name** | **Focus** |\n",
        "|----------|----------|-----------|\n",
        "| **CS** | Computer Science | Algorithms, systems, computational theory |\n",
        "| **IS** | Information Systems | Business processes, organizational technology |\n",
        "| **IT** | Information Technology | Infrastructure, operations, deployment |\n",
        "\n",
        "### üî¨ Subfields by Discipline\n",
        "\n",
        "#### Computer Science (CS)\n",
        "**AI/ML** (AI/Machine Learning), **CLOUDCS** (Cloud Computing), **CV** (Computer Vision), **NLP** (Natural Language Processing), **SE** (Software Engineering), **SEC** (Security)\n",
        "\n",
        "#### Information Systems (IS)  \n",
        "**BPM** (Business Process Management), **DT** (Digital Transformation), **GOV** (IT Governance), **HIS** (Health Information Systems), **KM** (Knowledge Management)\n",
        "\n",
        "#### Information Technology (IT)\n",
        "**CLOUDIT** (Cloud IT), **DEVOPS** (Development Operations), **EMERGING** (Emerging Technologies), **RISK** (Risk Management)\n",
        "\n",
        "### üìä Research Methodologies\n",
        "| **Code** | **Name** | **Description** |\n",
        "|----------|----------|-----------------|\n",
        "| **QUAL** | Qualitative | Interviews, case studies, ethnography |\n",
        "| **QUANT** | Quantitative | Experiments, surveys, statistical analysis |\n",
        "| **MIXED** | Mixed Methods | Combination of qualitative and quantitative |\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üõ†Ô∏è Section 1: Setup\n",
        "\n",
        "## Step 1: Working Directory Setup\n",
        "Run this cell first to ensure the notebook can find all model files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÇ Current working directory: /Users/aanandprabhu/Desktop/development/NLP-Project/Notebooks\n",
            "‚úÖ Changed to project root: /Users/aanandprabhu/Desktop/development/NLP-Project\n",
            "‚úÖ Found Artefacts/current directory\n",
            "‚úÖ Found 10 .pkl files\n"
          ]
        }
      ],
      "source": [
        "# üîß Working Directory Setup\n",
        "import os\n",
        "import sys\n",
        "\n",
        "print(f\"üìÇ Current working directory: {os.getcwd()}\")\n",
        "\n",
        "# Navigate to project root if needed\n",
        "if not os.path.exists('Artefacts/current'):\n",
        "    possible_paths = [\n",
        "        '.',  # Already in project root\n",
        "        '..',  # One level up\n",
        "        '../..',  # Two levels up\n",
        "        '../../..',  # Three levels up\n",
        "        '/Users/aanandprabhu/Desktop/development/NLP-Project',  # Absolute path\n",
        "    ]\n",
        "    \n",
        "    project_root = None\n",
        "    for path in possible_paths:\n",
        "        if os.path.exists(os.path.join(path, 'Artefacts/current')):\n",
        "            project_root = os.path.abspath(path)\n",
        "            break\n",
        "    \n",
        "    if project_root:\n",
        "        os.chdir(project_root)\n",
        "        print(f\"‚úÖ Changed to project root: {os.getcwd()}\")\n",
        "    else:\n",
        "        print(\"‚ùå Could not find project root directory!\")\n",
        "\n",
        "# Verify we can see the required files\n",
        "if os.path.exists('Artefacts/current'):\n",
        "    print(\"‚úÖ Found Artefacts/current directory\")\n",
        "    pkl_files = []\n",
        "    for root, dirs, files in os.walk('Artefacts/current'):\n",
        "        pkl_files.extend([os.path.join(root, f) for f in files if f.endswith('.pkl')])\n",
        "    print(f\"‚úÖ Found {len(pkl_files)} .pkl files\")\n",
        "else:\n",
        "    print(\"‚ùå Cannot find Artefacts/current directory\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 2: Import Libraries, Define Classes & Load Models\n",
        "This cell imports all required libraries and defines the OptimizedFeatureExtractor class needed for model loading. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ Loading classification models...\n",
            "‚úÖ Loaded discipline\n",
            "‚úÖ Loaded cs_subfield\n",
            "‚úÖ Loaded is_subfield\n",
            "‚úÖ Loaded it_subfield\n",
            "‚úÖ Loaded methodology\n",
            "\n",
            "üéâ Model loading complete! Successfully loaded 5/5 models\n",
            "üöÄ All models loaded successfully! Ready for classification.\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from typing import Dict, List, Tuple, Optional, Union\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For progress bars\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Required imports for model loading\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# Define the OptimizedFeatureExtractor class (required for model loading)\n",
        "class OptimizedFeatureExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Optimized feature extractor for academic paper classification.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.vectorizers = {\n",
        "            'tfidf': TfidfVectorizer(\n",
        "                max_features=3000,\n",
        "                ngram_range=(1, 2),\n",
        "                stop_words='english',\n",
        "                lowercase=True,\n",
        "                min_df=2,\n",
        "                max_df=0.95\n",
        "            )\n",
        "        }\n",
        "        self.is_fitted = False\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit the feature extractor.\"\"\"\n",
        "        for name, vectorizer in self.vectorizers.items():\n",
        "            vectorizer.fit(X)\n",
        "        self.is_fitted = True\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform texts to features.\"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Feature extractor must be fitted first\")\n",
        "        \n",
        "        features = []\n",
        "        for name, vectorizer in self.vectorizers.items():\n",
        "            features.append(vectorizer.transform(X))\n",
        "        \n",
        "        if len(features) == 1:\n",
        "            return features[0]\n",
        "        else:\n",
        "            return hstack(features)\n",
        "    \n",
        "    def fit_transform(self, X, y=None):\n",
        "        \"\"\"Fit and transform in one step.\"\"\"\n",
        "        return self.fit(X, y).transform(X)\n",
        "    \n",
        "    def get_feature_names(self):\n",
        "        \"\"\"Get all feature names\"\"\"\n",
        "        names = []\n",
        "        for name, vectorizer in self.vectorizers.items():\n",
        "            names.extend([f\"{name}_{n}\" for n in vectorizer.get_feature_names_out()])\n",
        "        return names\n",
        "\n",
        "# Define model paths\n",
        "MODEL_PATHS = {\n",
        "    'discipline': 'Artefacts/current/discipline_classifier_v7/discipline_pipeline_v7.pkl',\n",
        "    'cs_subfield': 'Artefacts/current/cs_subfield_classifier_v7/cs_subfield_pipeline_v7.pkl',\n",
        "    'is_subfield': 'Artefacts/current/is_subfield_classifier_v7/is_subfield_pipeline_v7.pkl',\n",
        "    'it_subfield': 'Artefacts/current/it_subfield_classifier_v7/it_subfield_pipeline_v7.pkl',\n",
        "    'methodology': 'Artefacts/current/methodology_classifier_v7/methodology_pipeline_v7.pkl'\n",
        "}\n",
        "\n",
        "# Load all models\n",
        "print(\"ü§ñ Loading classification models...\")\n",
        "models = {}\n",
        "successful_loads = 0\n",
        "\n",
        "for model_name, path in MODEL_PATHS.items():\n",
        "    try:\n",
        "        models[model_name] = joblib.load(path)\n",
        "        print(f\"‚úÖ Loaded {model_name}\")\n",
        "        successful_loads += 1\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading {model_name}: {e}\")\n",
        "        # Try alternative artifacts approach\n",
        "        artifact_path = f'Artefacts/current/{model_name}_classifier_v7/artifacts_v7.pkl'\n",
        "        try:\n",
        "            artifacts = joblib.load(artifact_path)\n",
        "            models[model_name] = {\n",
        "                'feature_extractor': artifacts['feature_extractor'],\n",
        "                'model': artifacts['model'],\n",
        "                'label_encoder': artifacts['label_encoder']\n",
        "            }\n",
        "            print(f\"‚úÖ Loaded {model_name} from artifacts\")\n",
        "            successful_loads += 1\n",
        "        except Exception as e2:\n",
        "            print(f\"‚ùå Failed to load {model_name} artifacts: {e2}\")\n",
        "            models[model_name] = None\n",
        "\n",
        "print(f\"\\nüéâ Model loading complete! Successfully loaded {successful_loads}/{len(MODEL_PATHS)} models\")\n",
        "\n",
        "if successful_loads == 0:\n",
        "    print(\"‚ö†Ô∏è  No models loaded successfully. Please check that you're in the correct directory and that model files exist.\")\n",
        "elif successful_loads < len(MODEL_PATHS):\n",
        "    print(\"‚ö†Ô∏è  Some models failed to load. Classification may not work for all disciplines.\")\n",
        "else:\n",
        "    print(\"üöÄ All models loaded successfully! Ready for classification.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üß† Section 2: Core Classification Functions\n",
        "\n",
        "## Step 3: Define Classification Functions\n",
        "These functions handle the actual paper classification with confidence scores and layman explanations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Core functions defined successfully!\n"
          ]
        }
      ],
      "source": [
        "def preprocess_text(text: str) -> str:\n",
        "    \"\"\"Clean and preprocess text input.\"\"\"\n",
        "    if pd.isna(text) or text is None:\n",
        "        return \"\"\n",
        "    return str(text).strip()\n",
        "\n",
        "def combine_title_abstract(title: str, abstract: str) -> str:\n",
        "    \"\"\"Combine title and abstract with proper formatting.\"\"\"\n",
        "    title = preprocess_text(title)\n",
        "    abstract = preprocess_text(abstract)\n",
        "    combined = f\"{title} {abstract}\".strip()\n",
        "    if not combined:\n",
        "        combined = \"No content available\"\n",
        "    return combined\n",
        "\n",
        "def get_prediction_confidence(model_dict, X) -> Tuple[str, float, np.ndarray]:\n",
        "    \"\"\"Get prediction with confidence score from model dictionary.\"\"\"\n",
        "    feature_extractor = model_dict['feature_extractor']\n",
        "    model = model_dict['model']\n",
        "    label_encoder = model_dict['label_encoder']\n",
        "    \n",
        "    # Transform input to features\n",
        "    X_features = feature_extractor.transform(X)\n",
        "    \n",
        "    # Get prediction and probabilities\n",
        "    prediction_encoded = model.predict(X_features)[0]\n",
        "    prediction = label_encoder.inverse_transform([prediction_encoded])[0]\n",
        "    \n",
        "    # Get probability scores\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        proba = model.predict_proba(X_features)[0]\n",
        "        confidence = float(np.max(proba))\n",
        "    else:\n",
        "        proba = np.array([1.0])\n",
        "        confidence = 1.0\n",
        "    \n",
        "    return prediction, confidence, proba\n",
        "\n",
        "def generate_layman_explanation(discipline, subfield, methodology, confidences):\n",
        "    \"\"\"Generate a simple, layman-friendly explanation of the classification.\"\"\"\n",
        "    \n",
        "    # Discipline explanations\n",
        "    discipline_explanations = {\n",
        "        'CS': \"focuses on technical computer science topics like algorithms and software systems\",\n",
        "        'IS': \"focuses on how technology is used in business and organizational contexts\", \n",
        "        'IT': \"focuses on practical technology implementation and infrastructure management\"\n",
        "    }\n",
        "    \n",
        "    # Subfield explanations\n",
        "    subfield_explanations = {\n",
        "        # CS subfields\n",
        "        'AI/ML': \"dealing with artificial intelligence and machine learning\",\n",
        "        'CLOUDCS': \"about cloud computing systems and distributed computing\",\n",
        "        'CV': \"about computer vision and image processing\",\n",
        "        'NLP': \"about natural language processing and text analysis\",\n",
        "        'SE': \"about software engineering and development practices\",\n",
        "        'SEC': \"about cybersecurity and information security\",\n",
        "        \n",
        "        # IS subfields\n",
        "        'BPM': \"about business process management and workflow optimization\",\n",
        "        'DT': \"about digital transformation in organizations\",\n",
        "        'GOV': \"about IT governance and technology policy\",\n",
        "        'HIS': \"about health information systems and medical technology\",\n",
        "        'KM': \"about knowledge management and information systems\",\n",
        "        \n",
        "        # IT subfields\n",
        "        'CLOUDIT': \"about cloud infrastructure and deployment\",\n",
        "        'DEVOPS': \"about development operations and automation\",\n",
        "        'EMERGING': \"about emerging technologies like IoT or blockchain\",\n",
        "        'RISK': \"about IT risk management and security operations\"\n",
        "    }\n",
        "    \n",
        "    # Methodology explanations\n",
        "    methodology_explanations = {\n",
        "        'QUAL': \"using qualitative research methods (interviews, case studies, observations)\",\n",
        "        'QUANT': \"using quantitative research methods (experiments, surveys, statistical analysis)\",\n",
        "        'MIXED': \"using a combination of both qualitative and quantitative research methods\"\n",
        "    }\n",
        "    \n",
        "    # Build the explanation\n",
        "    disc_conf = confidences.get('discipline', 0)\n",
        "    \n",
        "    explanation = f\"This paper {discipline_explanations.get(discipline, 'appears to be academic research')}\"\n",
        "    \n",
        "    if subfield and subfield != 'Unknown':\n",
        "        explanation += f\", specifically {subfield_explanations.get(subfield, f'in the {subfield} area')}\"\n",
        "    \n",
        "    if methodology and methodology != 'Unknown':\n",
        "        explanation += f\", {methodology_explanations.get(methodology, f'using {methodology} methods')}\"\n",
        "    \n",
        "    explanation += \".\"\n",
        "    \n",
        "    # Add confidence note if low\n",
        "    if disc_conf < 0.7:\n",
        "        explanation += \" (Note: The classifier has some uncertainty about this classification.)\"\n",
        "    \n",
        "    return explanation\n",
        "\n",
        "print(\"‚úÖ Core functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Main classification functions ready!\n"
          ]
        }
      ],
      "source": [
        "def classify_paper(title: str, abstract: str, include_layman_explanation: bool = True) -> Dict:\n",
        "    \"\"\"\n",
        "    Classify a single academic paper.\n",
        "    \n",
        "    Args:\n",
        "        title: Paper title\n",
        "        abstract: Paper abstract\n",
        "        include_layman_explanation: Whether to include a simple explanation\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with classification results including layman explanation\n",
        "    \"\"\"\n",
        "    results = {\n",
        "        'input': {'title': title, 'abstract': abstract[:200] + '...' if len(abstract) > 200 else abstract},\n",
        "        'predictions': {}, 'confidence_scores': {}, 'probability_distributions': {},\n",
        "        'warnings': []\n",
        "    }\n",
        "    \n",
        "    # Combine title and abstract\n",
        "    combined_text = combine_title_abstract(title, abstract)\n",
        "    \n",
        "    if combined_text == \"No content available\":\n",
        "        results['warnings'].append(\"Empty or missing title and abstract\")\n",
        "        results['predictions'] = {'discipline': 'Unknown', 'subfield': 'Unknown', 'methodology': 'Unknown'}\n",
        "        if include_layman_explanation:\n",
        "            results['layman_explanation'] = \"Cannot classify this paper because no title or abstract was provided.\"\n",
        "        return results\n",
        "    \n",
        "    # Step 1: Predict Discipline\n",
        "    if models['discipline'] is not None:\n",
        "        disc_pred, disc_conf, disc_proba = get_prediction_confidence(models['discipline'], [combined_text])\n",
        "        results['predictions']['discipline'] = disc_pred\n",
        "        results['confidence_scores']['discipline'] = disc_conf\n",
        "        \n",
        "        if 'label_encoder' in models['discipline']:\n",
        "            classes = models['discipline']['label_encoder'].classes_\n",
        "            results['probability_distributions']['discipline'] = {\n",
        "                cls: float(prob) for cls, prob in zip(classes, disc_proba)\n",
        "            }\n",
        "    else:\n",
        "        results['predictions']['discipline'] = 'Error'\n",
        "        results['warnings'].append(\"Discipline classifier not loaded\")\n",
        "        disc_pred = None\n",
        "    \n",
        "    # Step 2: Predict Subfield based on Discipline\n",
        "    if disc_pred in ['CS', 'IS', 'IT']:\n",
        "        subfield_model_name = f\"{disc_pred.lower()}_subfield\"\n",
        "        if models[subfield_model_name] is not None:\n",
        "            sub_pred, sub_conf, sub_proba = get_prediction_confidence(models[subfield_model_name], [combined_text])\n",
        "            results['predictions']['subfield'] = sub_pred\n",
        "            results['confidence_scores']['subfield'] = sub_conf\n",
        "            \n",
        "            if 'label_encoder' in models[subfield_model_name]:\n",
        "                classes = models[subfield_model_name]['label_encoder'].classes_\n",
        "                results['probability_distributions']['subfield'] = {\n",
        "                    cls: float(prob) for cls, prob in zip(classes, sub_proba)\n",
        "                }\n",
        "        else:\n",
        "            results['predictions']['subfield'] = 'Error'\n",
        "    else:\n",
        "        results['predictions']['subfield'] = 'Unknown'\n",
        "    \n",
        "    # Step 3: Predict Methodology\n",
        "    if models['methodology'] is not None:\n",
        "        meth_pred, meth_conf, meth_proba = get_prediction_confidence(models['methodology'], [combined_text])\n",
        "        results['predictions']['methodology'] = meth_pred\n",
        "        results['confidence_scores']['methodology'] = meth_conf\n",
        "        \n",
        "        if 'label_encoder' in models['methodology']:\n",
        "            classes = models['methodology']['label_encoder'].classes_\n",
        "            results['probability_distributions']['methodology'] = {\n",
        "                cls: float(prob) for cls, prob in zip(classes, meth_proba)\n",
        "            }\n",
        "    else:\n",
        "        results['predictions']['methodology'] = 'Error'\n",
        "    \n",
        "    # Add layman's explanation\n",
        "    if include_layman_explanation:\n",
        "        results['layman_explanation'] = generate_layman_explanation(\n",
        "            results['predictions']['discipline'],\n",
        "            results['predictions']['subfield'],\n",
        "            results['predictions']['methodology'],\n",
        "            results['confidence_scores']\n",
        "        )\n",
        "    \n",
        "    return results\n",
        "\n",
        "def classify_batch(df: pd.DataFrame, title_col: str = 'Title', abstract_col: str = 'Abstract', \n",
        "                  show_progress: bool = True) -> pd.DataFrame:\n",
        "    \"\"\"Classify multiple papers from a DataFrame.\"\"\"\n",
        "    results_list = []\n",
        "    \n",
        "    iterator = tqdm(df.iterrows(), total=len(df)) if show_progress else df.iterrows()\n",
        "    \n",
        "    for idx, row in iterator:\n",
        "        title = row.get(title_col, '')\n",
        "        abstract = row.get(abstract_col, '')\n",
        "        \n",
        "        result = classify_paper(title, abstract)\n",
        "        \n",
        "        flat_result = {\n",
        "            'index': idx,\n",
        "            'title': title,\n",
        "            'abstract': abstract[:100] + '...' if len(str(abstract)) > 100 else abstract,\n",
        "            'discipline': result['predictions'].get('discipline', 'Unknown'),\n",
        "            'discipline_confidence': result['confidence_scores'].get('discipline', 0),\n",
        "            'subfield': result['predictions'].get('subfield', 'Unknown'),\n",
        "            'subfield_confidence': result['confidence_scores'].get('subfield', 0),\n",
        "            'methodology': result['predictions'].get('methodology', 'Unknown'),\n",
        "            'methodology_confidence': result['confidence_scores'].get('methodology', 0),\n",
        "            'layman_explanation': result.get('layman_explanation', ''),\n",
        "            'warnings': '; '.join(result['warnings']) if result['warnings'] else 'None'\n",
        "        }\n",
        "        \n",
        "        results_list.append(flat_result)\n",
        "    \n",
        "    return pd.DataFrame(results_list)\n",
        "\n",
        "print(\"‚úÖ Main classification functions ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üéØ Section 3: Examples & Usage\n",
        "\n",
        "## Single Paper Classification Examples\n",
        "Try these examples to see how the classifier works with different types of papers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üî¨ Example 1: Natural Language Processing Research\n",
            "============================================================\n",
            "üìä Classification Results:\n",
            "Discipline: CS (confidence: 1.00)\n",
            "Subfield: NLP (confidence: 0.96)\n",
            "Methodology: QUANT (confidence: 0.83)\n",
            "\n",
            "üí¨ Plain English: This paper focuses on technical computer science topics like algorithms and software systems, specifically about natural language processing and text analysis, using quantitative research methods (experiments, surveys, statistical analysis).\n",
            "\n",
            "============================================================\n",
            "‚öôÔ∏è Example 2: DevOps Infrastructure Research\n",
            "üìä Classification Results:\n",
            "Discipline: IT (confidence: 0.99)\n",
            "Subfield: DEVOPS (confidence: 1.00)\n",
            "Methodology: MIXED (confidence: 0.84)\n",
            "\n",
            "üí¨ Plain English: This paper focuses on practical technology implementation and infrastructure management, specifically about development operations and automation, using a combination of both qualitative and quantitative research methods.\n",
            "\n",
            "============================================================\n",
            "üèõÔ∏è Example 3: Government IT Research\n",
            "üìä Classification Results:\n",
            "Discipline: IS (confidence: 1.00)\n",
            "Subfield: GOV (confidence: 0.90)\n",
            "Methodology: QUAL (confidence: 0.99)\n",
            "\n",
            "üí¨ Plain English: This paper focuses on how technology is used in business and organizational contexts, specifically about IT governance and technology policy, using qualitative research methods (interviews, case studies, observations).\n",
            "\n",
            "üéØ These examples show how the classifier handles different disciplines, subfields, and methodologies!\n"
          ]
        }
      ],
      "source": [
        "# Example 1: CS/NLP/QUANT\n",
        "print(\"üî¨ Example 1: Natural Language Processing Research\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result1 = classify_paper(\n",
        "    title=\"Deep Learning for Natural Language Processing: A Survey\",\n",
        "    abstract=\"\"\"This paper provides a comprehensive survey of deep learning techniques \n",
        "    applied to natural language processing tasks. We review recent advances in neural \n",
        "    architectures including transformers, BERT, and GPT models. Our analysis covers \n",
        "    both theoretical foundations and practical applications, with empirical comparisons \n",
        "    of model performance across various benchmarks.\"\"\"\n",
        ")\n",
        "\n",
        "print(f\"üìä Classification Results:\")\n",
        "print(f\"Discipline: {result1['predictions']['discipline']} (confidence: {result1['confidence_scores'].get('discipline', 0):.2f})\")\n",
        "print(f\"Subfield: {result1['predictions']['subfield']} (confidence: {result1['confidence_scores'].get('subfield', 0):.2f})\")\n",
        "print(f\"Methodology: {result1['predictions']['methodology']} (confidence: {result1['confidence_scores'].get('methodology', 0):.2f})\")\n",
        "print(f\"\\nüí¨ Plain English: {result1['layman_explanation']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# Example 2: IT/DEVOPS/MIXED\n",
        "print(\"‚öôÔ∏è Example 2: DevOps Infrastructure Research\")\n",
        "\n",
        "result2 = classify_paper(\n",
        "    title=\"Automated Testing in Continuous Integration Pipelines: A Mixed-Methods Analysis\",\n",
        "    abstract=\"\"\"This study evaluates automated testing practices in CI/CD pipelines through \n",
        "    a comprehensive mixed-methods approach. We collected performance metrics from 150 production \n",
        "    deployments across 10 organizations, measuring test execution times, failure rates, and \n",
        "    deployment frequency. Additionally, we conducted semi-structured interviews with 30 DevOps \n",
        "    engineers to understand their experiences with test automation challenges and best practices.\"\"\"\n",
        ")\n",
        "\n",
        "print(f\"üìä Classification Results:\")\n",
        "print(f\"Discipline: {result2['predictions']['discipline']} (confidence: {result2['confidence_scores'].get('discipline', 0):.2f})\")\n",
        "print(f\"Subfield: {result2['predictions']['subfield']} (confidence: {result2['confidence_scores'].get('subfield', 0):.2f})\")\n",
        "print(f\"Methodology: {result2['predictions']['methodology']} (confidence: {result2['confidence_scores'].get('methodology', 0):.2f})\")\n",
        "print(f\"\\nüí¨ Plain English: {result2['layman_explanation']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# Example 3: IS/GOV/QUAL\n",
        "print(\"üèõÔ∏è Example 3: Government IT Research\")\n",
        "\n",
        "result3 = classify_paper(\n",
        "    title=\"Barriers to Digital Transformation in Municipal Government: A Qualitative Investigation\",\n",
        "    abstract=\"\"\"This qualitative study examines organizational and political barriers to digital \n",
        "    transformation in local government. Through ethnographic observation and in-depth interviews \n",
        "    with 45 municipal officials across three cities, we identify key challenges including legacy \n",
        "    system dependencies, bureaucratic resistance, and citizen privacy concerns. Our grounded theory \n",
        "    analysis reveals how institutional factors shape technology adoption decisions in public sector contexts.\"\"\"\n",
        ")\n",
        "\n",
        "print(f\"üìä Classification Results:\")\n",
        "print(f\"Discipline: {result3['predictions']['discipline']} (confidence: {result3['confidence_scores'].get('discipline', 0):.2f})\")\n",
        "print(f\"Subfield: {result3['predictions']['subfield']} (confidence: {result3['confidence_scores'].get('subfield', 0):.2f})\")\n",
        "print(f\"Methodology: {result3['predictions']['methodology']} (confidence: {result3['confidence_scores'].get('methodology', 0):.2f})\")\n",
        "print(f\"\\nüí¨ Plain English: {result3['layman_explanation']}\")\n",
        "\n",
        "print(\"\\nüéØ These examples show how the classifier handles different disciplines, subfields, and methodologies!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Batch Processing Example\n",
        "Classify multiple papers at once from a DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Testing classification system...\n",
            "‚úÖ Classification system is working!\n",
            "Test result: CS/SE/QUANT\n",
            "üí¨ Test explanation: This paper focuses on technical computer science topics like algorithms and software systems, specifically about software engineering and development practices, using quantitative research methods (experiments, surveys, statistical analysis). (Note: The classifier has some uncertainty about this classification.)\n",
            "\n",
            "üéØ Ready for examples and your own papers!\n"
          ]
        }
      ],
      "source": [
        "# üß™ Test to verify everything is working\n",
        "print(\"üß™ Testing classification system...\")\n",
        "\n",
        "try:\n",
        "    test_result = classify_paper(\n",
        "        title=\"Test Classification\",\n",
        "        abstract=\"This is a simple test to verify the classification system is working properly.\"\n",
        "    )\n",
        "    print(\"‚úÖ Classification system is working!\")\n",
        "    print(f\"Test result: {test_result['predictions']['discipline']}/{test_result['predictions']['subfield']}/{test_result['predictions']['methodology']}\")\n",
        "    print(f\"üí¨ Test explanation: {test_result['layman_explanation']}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Classification test failed: {e}\")\n",
        "    print(\"Please check that all setup cells (1-4) ran successfully.\")\n",
        "\n",
        "print(\"\\nüéØ Ready for examples and your own papers!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Batch Classification Results:\n",
            "================================================================================\n",
            "\n",
            "üìÑ Paper 1: Machine Learning in Healthcare: A Systematic Revie...\n",
            "   Classification: CS/AI/ML/QUAL\n",
            "   Confidence: D:0.94 | S:0.97 | M:0.82\n",
            "   üí¨ Plain English: This paper focuses on technical computer science topics like algorithms and software systems, specifically dealing with artificial intelligence and machine learning, using qualitative research methods (interviews, case studies, observations).\n",
            "\n",
            "üìÑ Paper 2: Enterprise Resource Planning Implementation: A Cas...\n",
            "   Classification: IS/BPM/QUAL\n",
            "   Confidence: D:1.00 | S:1.00 | M:0.54\n",
            "   üí¨ Plain English: This paper focuses on how technology is used in business and organizational contexts, specifically about business process management and workflow optimization, using qualitative research methods (interviews, case studies, observations).\n",
            "\n",
            "üìÑ Paper 3: Network Security in IoT Devices: Vulnerabilities a...\n",
            "   Classification: CS/SEC/QUANT\n",
            "   Confidence: D:0.78 | S:0.98 | M:0.62\n",
            "   üí¨ Plain English: This paper focuses on technical computer science topics like algorithms and software systems, specifically about cybersecurity and information security, using quantitative research methods (experiments, surveys, statistical analysis).\n",
            "\n",
            "‚úÖ Successfully classified 3 papers!\n"
          ]
        }
      ],
      "source": [
        "# Create sample dataset for batch processing\n",
        "sample_papers = pd.DataFrame([\n",
        "    {\n",
        "        'Title': 'Machine Learning in Healthcare: A Systematic Review',\n",
        "        'Abstract': 'This systematic review examines the application of machine learning techniques in healthcare settings. We analyzed 150 papers published between 2018-2023, identifying key trends and challenges in clinical decision support systems.'\n",
        "    },\n",
        "    {\n",
        "        'Title': 'Enterprise Resource Planning Implementation: A Case Study',\n",
        "        'Abstract': 'This paper presents an in-depth case study of ERP implementation in a Fortune 500 company. Through interviews with 30 stakeholders and analysis of organizational documents, we identify critical success factors.'\n",
        "    },\n",
        "    {\n",
        "        'Title': 'Network Security in IoT Devices: Vulnerabilities and Solutions',\n",
        "        'Abstract': 'We present a comprehensive analysis of security vulnerabilities in Internet of Things devices. Our study includes penetration testing of 50 commercial IoT devices and proposes a new security framework.'\n",
        "    }\n",
        "])\n",
        "\n",
        "print(\"üìä Batch Classification Results:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Classify all papers\n",
        "batch_results = classify_batch(sample_papers, show_progress=False)\n",
        "\n",
        "# Display results with layman explanations\n",
        "for idx, row in batch_results.iterrows():\n",
        "    print(f\"\\nüìÑ Paper {idx + 1}: {row['title'][:50]}...\")\n",
        "    print(f\"   Classification: {row['discipline']}/{row['subfield']}/{row['methodology']}\")\n",
        "    print(f\"   Confidence: D:{row['discipline_confidence']:.2f} | S:{row['subfield_confidence']:.2f} | M:{row['methodology_confidence']:.2f}\")\n",
        "    print(f\"   üí¨ Plain English: {row['layman_explanation']}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Successfully classified {len(batch_results)} papers!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üöÄ Section 4: Classify Your Own Papers\n",
        "\n",
        "## Option A: Single Paper Classification\n",
        "Ready to test! Sample data is provided below - just run the cell to see classification results.\n",
        "\n",
        "**To use your own paper**: Modify the variables in the cell below and run it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Classifying your paper...\n",
            "============================================================\n",
            "üìÑ Title: Machine Learning Approaches for Cybersecurity Threat Detection in IoT Networks\n",
            "\n",
            "üìä Classification Results:\n",
            "Discipline: CS (confidence: 1.00)\n",
            "Subfield: SEC (confidence: 1.00)\n",
            "Methodology: QUANT (confidence: 1.00)\n",
            "\n",
            "üí¨ Plain English Explanation:\n",
            "This paper focuses on technical computer science topics like algorithms and software systems, specifically about cybersecurity and information security, using quantitative research methods (experiments, surveys, statistical analysis).\n",
            "\n",
            "üìà Detailed Confidence Scores:\n",
            "  Discipline: 0.999 (üü¢ High)\n",
            "  Subfield: 0.997 (üü¢ High)\n",
            "  Methodology: 0.997 (üü¢ High)\n"
          ]
        }
      ],
      "source": [
        "# üìù ENTER YOUR PAPER DETAILS HERE\n",
        "# (Sample data provided below - replace with your own paper!)\n",
        "your_title = \"Machine Learning Approaches for Cybersecurity Threat Detection in IoT Networks\"\n",
        "your_abstract = \"\"\"Internet of Things (IoT) networks face increasing cybersecurity threats due to their distributed nature and resource constraints. This study proposes a novel machine learning framework for real-time threat detection in IoT environments. We developed and evaluated three ML models: Random Forest, Support Vector Machines, and Deep Neural Networks using a dataset of 50,000 network traffic samples from simulated IoT deployments. Our experimental results demonstrate that the ensemble approach combining all three models achieves 96.7% accuracy in detecting malicious activities, including DDoS attacks, malware propagation, and unauthorized access attempts. The proposed system operates with low latency (average 15ms response time) making it suitable for real-time deployment. Feature importance analysis reveals that packet size distribution, connection frequency, and payload entropy are the most significant indicators of malicious behavior. This research contributes to the field by providing a practical and efficient solution for securing IoT networks against evolving cyber threats.\"\"\"\n",
        "\n",
        "# Classify your paper\n",
        "print(\"üîç Classifying your paper...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "your_result = classify_paper(your_title, your_abstract)\n",
        "\n",
        "print(f\"üìÑ Title: {your_title}\")\n",
        "print(f\"\\nüìä Classification Results:\")\n",
        "print(f\"Discipline: {your_result['predictions']['discipline']} (confidence: {your_result['confidence_scores'].get('discipline', 0):.2f})\")\n",
        "print(f\"Subfield: {your_result['predictions']['subfield']} (confidence: {your_result['confidence_scores'].get('subfield', 0):.2f})\")\n",
        "print(f\"Methodology: {your_result['predictions']['methodology']} (confidence: {your_result['confidence_scores'].get('methodology', 0):.2f})\")\n",
        "\n",
        "print(f\"\\nüí¨ Plain English Explanation:\")\n",
        "print(f\"{your_result['layman_explanation']}\")\n",
        "\n",
        "if your_result['warnings']:\n",
        "    print(f\"\\n‚ö†Ô∏è Warnings: {'; '.join(your_result['warnings'])}\")\n",
        "\n",
        "# Show confidence breakdown\n",
        "print(f\"\\nüìà Detailed Confidence Scores:\")\n",
        "for level, conf in your_result['confidence_scores'].items():\n",
        "    status = \"üü¢ High\" if conf > 0.8 else \"üü° Medium\" if conf > 0.6 else \"üî¥ Low\"\n",
        "    print(f\"  {level.capitalize()}: {conf:.3f} ({status})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Option B: Batch Classification from CSV\n",
        "Load and classify papers from your own CSV file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üí° Uncomment the code above and modify the path to classify your own CSV file!\n",
            "üìã Your CSV should have columns for 'Title' and 'Abstract'\n",
            "üéØ Results will include layman explanations for each paper\n"
          ]
        }
      ],
      "source": [
        "# üìÅ LOAD YOUR CSV FILE HERE\n",
        "# Uncomment and modify the path below to use your own data\n",
        "\n",
        "# your_csv_path = 'path/to/your/papers.csv'\n",
        "# your_papers = pd.read_csv(your_csv_path)\n",
        "\n",
        "# # Classify all papers (adjust column names if needed)\n",
        "# your_results = classify_batch(\n",
        "#     your_papers,\n",
        "#     title_col='Title',     # Change this to your title column name\n",
        "#     abstract_col='Abstract' # Change this to your abstract column name\n",
        "# )\n",
        "\n",
        "# # Save results\n",
        "# your_results.to_csv('classified_papers.csv', index=False)\n",
        "# print(f\"‚úÖ Classified {len(your_results)} papers and saved to 'classified_papers.csv'\")\n",
        "\n",
        "# # Display summary\n",
        "# print(\"\\nüìä Classification Summary:\")\n",
        "# print(f\"Disciplines: {your_results['discipline'].value_counts().to_dict()}\")\n",
        "# print(f\"Methodologies: {your_results['methodology'].value_counts().to_dict()}\")\n",
        "\n",
        "print(\"üí° Uncomment the code above and modify the path to classify your own CSV file!\")\n",
        "print(\"üìã Your CSV should have columns for 'Title' and 'Abstract'\")\n",
        "print(\"üéØ Results will include layman explanations for each paper\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üìä Section 5: Advanced Features\n",
        "\n",
        "## Interactive Classification \n",
        "Ready to test! Sample data is provided below - just run the cell to see classification results.\n",
        "\n",
        "**To use your own paper**: Modify the variables in the cell below and run it.\n",
        "\n",
        "**Note**: `input()` functions don't work well in Jupyter notebooks, so this uses a variable-based approach instead.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéì Interactive Classification - Modify variables below and run this cell!\n",
            "===========================================================================\n",
            "üìÑ Classifying: Blockchain-based Supply Chain Management: A Systematic Review of Security and Transparency Challenges\n",
            "--------------------------------------------------\n",
            "üìä Classification Results:\n",
            "Discipline: IS (confidence: 63.25%)\n",
            "Subfield: BPM (confidence: 99.71%)\n",
            "Methodology: QUAL (confidence: 52.99%)\n",
            "\n",
            "üí¨ Plain English Explanation:\n",
            "This paper focuses on how technology is used in business and organizational contexts, specifically about business process management and workflow optimization, using qualitative research methods (interviews, case studies, observations). (Note: The classifier has some uncertainty about this classification.)\n",
            "\n",
            "üìà Confidence Breakdown:\n",
            "  Discipline: 0.633 (üü° Medium)\n",
            "  Subfield: 0.997 (üü¢ High)\n",
            "  Methodology: 0.530 (üî¥ Low)\n",
            "\n",
            "üîÑ To classify another paper: modify the variables above and run this cell again!\n"
          ]
        }
      ],
      "source": [
        "# üé™ Simple Interactive Classification\n",
        "# Note: input() functions don't work well in Jupyter notebooks\n",
        "# Instead, just modify the variables below and run this cell!\n",
        "\n",
        "print(\"üéì Interactive Classification - Modify variables below and run this cell!\")\n",
        "print(\"=\" * 75)\n",
        "\n",
        "# üìù MODIFY THESE VARIABLES FOR YOUR PAPER:\n",
        "# (Sample data provided below - replace with your own paper!)\n",
        "interactive_title = \"Blockchain-based Supply Chain Management: A Systematic Review of Security and Transparency Challenges\"\n",
        "interactive_abstract = \"\"\"This paper presents a comprehensive systematic review of blockchain technology applications in supply chain management, focusing on security and transparency challenges. We analyzed 127 peer-reviewed papers published between 2018-2023 from major databases including IEEE Xplore, ACM Digital Library, and ScienceDirect. Our methodology involved a structured literature review protocol with predefined inclusion and exclusion criteria. The review identifies key security vulnerabilities in blockchain-based supply chains, including smart contract exploits, consensus mechanism attacks, and privacy concerns. We found that while blockchain technology offers significant improvements in supply chain transparency and traceability, implementation challenges persist regarding scalability, interoperability, and energy consumption. The study contributes to the field by providing a taxonomic classification of security threats and proposing a framework for evaluating blockchain solutions in supply chain contexts. Our findings suggest that hybrid blockchain architectures combined with traditional database systems offer the most practical approach for current enterprise implementations.\"\"\"\n",
        "\n",
        "# üîç Classify the paper\n",
        "print(f\"üìÑ Classifying: {interactive_title}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "if interactive_title and interactive_title.strip() and len(interactive_title.strip()) > 5:\n",
        "    try:\n",
        "        result = classify_paper(interactive_title, interactive_abstract)\n",
        "        \n",
        "        print(f\"üìä Classification Results:\")\n",
        "        print(f\"Discipline: {result['predictions']['discipline']} (confidence: {result['confidence_scores'].get('discipline', 0):.2%})\")\n",
        "        print(f\"Subfield: {result['predictions']['subfield']} (confidence: {result['confidence_scores'].get('subfield', 0):.2%})\")\n",
        "        print(f\"Methodology: {result['predictions']['methodology']} (confidence: {result['confidence_scores'].get('methodology', 0):.2%})\")\n",
        "        \n",
        "        print(f\"\\nüí¨ Plain English Explanation:\")\n",
        "        print(f\"{result['layman_explanation']}\")\n",
        "        \n",
        "        # Show confidence breakdown\n",
        "        print(f\"\\nüìà Confidence Breakdown:\")\n",
        "        for level, conf in result['confidence_scores'].items():\n",
        "            status = \"üü¢ High\" if conf > 0.8 else \"üü° Medium\" if conf > 0.6 else \"üî¥ Low\"\n",
        "            print(f\"  {level.capitalize()}: {conf:.3f} ({status})\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Classification failed: {e}\")\n",
        "else:\n",
        "    print(\"üí° Please enter a valid paper title (more than 5 characters) and run again!\")\n",
        "\n",
        "print(\"\\nüîÑ To classify another paper: modify the variables above and run this cell again!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary & Key Features\n",
        "\n",
        "### ‚úÖ What This Notebook Provides:\n",
        "\n",
        "1. **üéØ Hierarchical Classification**: Discipline ‚Üí Subfield ‚Üí Methodology\n",
        "2. **üìä Confidence Scores**: For each prediction level  \n",
        "3. **üí¨ Layman Explanations**: Simple, plain-English interpretations\n",
        "4. **üîÑ Batch Processing**: Efficient classification of multiple papers\n",
        "5. **üìÅ CSV Export**: Save results for further analysis\n",
        "6. **üé™ Variable-Based Mode**: Easy classification by modifying variables\n",
        "\n",
        "### üîß Technical Details:\n",
        "\n",
        "- **Models**: Pre-trained v7.0 XGBoost classifiers  \n",
        "- **Features**: 3,000 TF-IDF features per classifier\n",
        "- **Setup**: 5 cells for complete initialization\n",
        "- **Validation**: Proper train/validation/test splits\n",
        "- **Anti-leakage**: No data contamination between sets\n",
        "- **Performance**: 92.32% discipline accuracy, 86.92% methodology accuracy\n",
        "\n",
        "### üéì Example Classifications:\n",
        "\n",
        "- **CS/NLP/QUANT**: Natural language processing with empirical analysis\n",
        "- **IT/DEVOPS/MIXED**: Infrastructure research with combined methods  \n",
        "- **IS/GOV/QUAL**: Government technology with qualitative approaches\n",
        "\n",
        "---\n",
        "\n",
        "**üöÄ Ready to classify your papers? Run cells 1-5 first, then start with Section 3 examples or jump to Section 4 for your own papers!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlp-bert",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
